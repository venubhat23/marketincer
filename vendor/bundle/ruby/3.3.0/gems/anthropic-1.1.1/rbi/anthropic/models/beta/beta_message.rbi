# typed: strong

module Anthropic
  module Models
    BetaMessage = Beta::BetaMessage

    module Beta
      class BetaMessage < Anthropic::Internal::Type::BaseModel
        OrHash =
          T.type_alias do
            T.any(Anthropic::Beta::BetaMessage, Anthropic::Internal::AnyHash)
          end

        # Unique object identifier.
        #
        # The format and length of IDs may change over time.
        sig { returns(String) }
        attr_accessor :id

        # Information about the container used in the request (for the code execution
        # tool)
        sig { returns(T.nilable(Anthropic::Beta::BetaContainer)) }
        attr_reader :container

        sig do
          params(
            container: T.nilable(Anthropic::Beta::BetaContainer::OrHash)
          ).void
        end
        attr_writer :container

        # Content generated by the model.
        #
        # This is an array of content blocks, each of which has a `type` that determines
        # its shape.
        #
        # Example:
        #
        # ```json
        # [{ "type": "text", "text": "Hi, I'm Claude." }]
        # ```
        #
        # If the request input `messages` ended with an `assistant` turn, then the
        # response `content` will continue directly from that last turn. You can use this
        # to constrain the model's output.
        #
        # For example, if the input `messages` were:
        #
        # ```json
        # [
        #   {
        #     "role": "user",
        #     "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
        #   },
        #   { "role": "assistant", "content": "The best answer is (" }
        # ]
        # ```
        #
        # Then the response `content` might be:
        #
        # ```json
        # [{ "type": "text", "text": "B)" }]
        # ```
        sig { returns(T::Array[Anthropic::Beta::BetaContentBlock::Variants]) }
        attr_accessor :content

        # The model that will complete your prompt.\n\nSee
        # [models](https://docs.anthropic.com/en/docs/models-overview) for additional
        # details and options.
        sig { returns(Anthropic::Model::Variants) }
        attr_accessor :model

        # Conversational role of the generated message.
        #
        # This will always be `"assistant"`.
        sig { returns(Symbol) }
        attr_accessor :role

        # The reason that we stopped.
        #
        # This may be one the following values:
        #
        # - `"end_turn"`: the model reached a natural stopping point
        # - `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
        # - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
        # - `"tool_use"`: the model invoked one or more tools
        #
        # In non-streaming mode this value is always non-null. In streaming mode, it is
        # null in the `message_start` event and non-null otherwise.
        sig do
          returns(T.nilable(Anthropic::Beta::BetaStopReason::TaggedSymbol))
        end
        attr_accessor :stop_reason

        # Which custom stop sequence was generated, if any.
        #
        # This value will be a non-null string if one of your custom stop sequences was
        # generated.
        sig { returns(T.nilable(String)) }
        attr_accessor :stop_sequence

        # Object type.
        #
        # For Messages, this is always `"message"`.
        sig { returns(Symbol) }
        attr_accessor :type

        # Billing and rate-limit usage.
        #
        # Anthropic's API bills and rate-limits by token counts, as tokens represent the
        # underlying cost to our systems.
        #
        # Under the hood, the API transforms requests into a format suitable for the
        # model. The model's output then goes through a parsing stage before becoming an
        # API response. As a result, the token counts in `usage` will not match one-to-one
        # with the exact visible content of an API request or response.
        #
        # For example, `output_tokens` will be non-zero, even for an empty string response
        # from Claude.
        #
        # Total input tokens in a request is the summation of `input_tokens`,
        # `cache_creation_input_tokens`, and `cache_read_input_tokens`.
        sig { returns(Anthropic::Beta::BetaUsage) }
        attr_reader :usage

        sig { params(usage: Anthropic::Beta::BetaUsage::OrHash).void }
        attr_writer :usage

        sig do
          params(
            id: String,
            container: T.nilable(Anthropic::Beta::BetaContainer::OrHash),
            content:
              T::Array[
                T.any(
                  Anthropic::Beta::BetaTextBlock::OrHash,
                  Anthropic::Beta::BetaToolUseBlock::OrHash,
                  Anthropic::Beta::BetaServerToolUseBlock::OrHash,
                  Anthropic::Beta::BetaWebSearchToolResultBlock::OrHash,
                  Anthropic::Beta::BetaCodeExecutionToolResultBlock::OrHash,
                  Anthropic::Beta::BetaMCPToolUseBlock::OrHash,
                  Anthropic::Beta::BetaMCPToolResultBlock::OrHash,
                  Anthropic::Beta::BetaContainerUploadBlock::OrHash,
                  Anthropic::Beta::BetaThinkingBlock::OrHash,
                  Anthropic::Beta::BetaRedactedThinkingBlock::OrHash
                )
              ],
            model: T.any(Anthropic::Model::OrSymbol, String),
            stop_reason: T.nilable(Anthropic::Beta::BetaStopReason::OrSymbol),
            stop_sequence: T.nilable(String),
            usage: Anthropic::Beta::BetaUsage::OrHash,
            role: Symbol,
            type: Symbol
          ).returns(T.attached_class)
        end
        def self.new(
          # Unique object identifier.
          #
          # The format and length of IDs may change over time.
          id:,
          # Information about the container used in the request (for the code execution
          # tool)
          container:,
          # Content generated by the model.
          #
          # This is an array of content blocks, each of which has a `type` that determines
          # its shape.
          #
          # Example:
          #
          # ```json
          # [{ "type": "text", "text": "Hi, I'm Claude." }]
          # ```
          #
          # If the request input `messages` ended with an `assistant` turn, then the
          # response `content` will continue directly from that last turn. You can use this
          # to constrain the model's output.
          #
          # For example, if the input `messages` were:
          #
          # ```json
          # [
          #   {
          #     "role": "user",
          #     "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
          #   },
          #   { "role": "assistant", "content": "The best answer is (" }
          # ]
          # ```
          #
          # Then the response `content` might be:
          #
          # ```json
          # [{ "type": "text", "text": "B)" }]
          # ```
          content:,
          # The model that will complete your prompt.\n\nSee
          # [models](https://docs.anthropic.com/en/docs/models-overview) for additional
          # details and options.
          model:,
          # The reason that we stopped.
          #
          # This may be one the following values:
          #
          # - `"end_turn"`: the model reached a natural stopping point
          # - `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
          # - `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
          # - `"tool_use"`: the model invoked one or more tools
          #
          # In non-streaming mode this value is always non-null. In streaming mode, it is
          # null in the `message_start` event and non-null otherwise.
          stop_reason:,
          # Which custom stop sequence was generated, if any.
          #
          # This value will be a non-null string if one of your custom stop sequences was
          # generated.
          stop_sequence:,
          # Billing and rate-limit usage.
          #
          # Anthropic's API bills and rate-limits by token counts, as tokens represent the
          # underlying cost to our systems.
          #
          # Under the hood, the API transforms requests into a format suitable for the
          # model. The model's output then goes through a parsing stage before becoming an
          # API response. As a result, the token counts in `usage` will not match one-to-one
          # with the exact visible content of an API request or response.
          #
          # For example, `output_tokens` will be non-zero, even for an empty string response
          # from Claude.
          #
          # Total input tokens in a request is the summation of `input_tokens`,
          # `cache_creation_input_tokens`, and `cache_read_input_tokens`.
          usage:,
          # Conversational role of the generated message.
          #
          # This will always be `"assistant"`.
          role: :assistant,
          # Object type.
          #
          # For Messages, this is always `"message"`.
          type: :message
        )
        end

        sig do
          override.returns(
            {
              id: String,
              container: T.nilable(Anthropic::Beta::BetaContainer),
              content: T::Array[Anthropic::Beta::BetaContentBlock::Variants],
              model: Anthropic::Model::Variants,
              role: Symbol,
              stop_reason:
                T.nilable(Anthropic::Beta::BetaStopReason::TaggedSymbol),
              stop_sequence: T.nilable(String),
              type: Symbol,
              usage: Anthropic::Beta::BetaUsage
            }
          )
        end
        def to_hash
        end
      end
    end
  end
end
